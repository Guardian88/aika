<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Training - Aika</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
</head>
<body>
<div id="header">
	<div>
		<div class="logo">
			<a href="index.html"></a>
		</div>
		<ul id="navigation">
			<li>
				<a href="index.html">Overall idea</a>
			</li>
			<li>
				<a href="inference.html">Inference</a>
			</li>
			<li class="active">
				<a href="training.html">Training</a>
			</li>
			<li>
				<a href="usage.html">Examples</a>
			</li>
			<li>
				<a href="resources.html">Resources</a>
			</li>
		</ul>
	</div>
</div>
<div id="contents">
	<div class="features">
		<h1>Training Mechanisms</h1>
		<p>
			The architectural framework of the Aika algorithm allows to implement a broad spectrum of diverse
			training mechanisms. These training mechanisms have their roots within the different artificial intelligence
			concepts that Aika brings together.
		</p>
		<h2>Supervised Learning with the Delta-Rule</h2>
		<p>
			A standard training method that can be used for the Aika network as well is the delta rule.
		</p>
		$$\Delta w_{ij} = \alpha (t_j - y_j) x_i$$
		<p>
			Where \(\alpha \) is the learn rate, \(t_j \) is the target output value, \(x_i \) is the input activation value
			and \(y_j \) is the actual output activation value. \(\Delta w_{ij}\) is the change that is added to the
			synapse weight after each training round. <br/>
			To perform supervised training on an input data set, the target values have to be assigned to the output
			activations. This can be done in the following way:

		</p>
		<div class="prettyprint-code">
        	<pre class="prettyprint">
				<code class="language-java">
			out.addInput(doc,
				new Activation.Builder()
					.setRange(0, 3)
					.setValue(0.0)
					.setTargetValue(1.0)
			);
        		</code>
			</pre>
		</div>
		<p>
			The actual training is the started using the method call doc.supervisedTraining.train().
		</p>

		<h2>Long-Term Potentiation and Long-Term Depression</h2>
		<p>
			Long-term Potentiation and Long-Term Depression are two learning mechanisms that occur naturally in the brain.
			Long-term Potentiation (LTP) is the process of a persistent strengthening of synapses based on recent
			patterns of activity. These are patterns of synaptic activity that produce a long-lasting increase in signal
			transmission between two neurons. The opposite of LTP is long-term depression, which produces a long-lasting
			decrease in synaptic strength. <br/>

			LTP also gave rise to Hebbs learning rule which can be summarized by the expression "What fires together
			wires together". <br/>

			The following is a formulaic description of Hebbian learning:
		</p>
			$$\Delta w_{ij} = \alpha x_i y_j$$
		<p>
			Where \(\alpha \) is the learn rate, \(x_i \) is the input activation value and \(y_j \) is the output
			activation value.
			The weakness of Hebbs learning rule is that the strength of synapses will increase infinitely, which means
			that networks based on the Hebb rule are unstable. <br/>

			The attempt to model LTP in Aika looks as follows:
		</p>
			$$\Delta w_{ij} = \alpha \sigma_{ij} x_i (1 - y_j) y_j h_j$$

			$$h_j = \Bigg \{ {{net_j \over g_j^+} \atop 1} {: conj(w_{ij}) \atop : disj(w_{ij})} $$

			$$g_j^+ = b_j + \sum\limits_{i=0,w_{ij} > 0.0}^N{w_{ij}}$$

		<p>
			Where \(\alpha \) is the learn rate, \(h_j\) is a measure for the pattern match of conjunctive synapses.
			\(net_j\) is the weighted sum and \(g_j^+\) is the sum of all positive
			weights. The term \((1 - y_j) \) is used to limit the strength of the synapse and prevent it from growing
			indefinitely. The term \(net_j \over g_j^+ \) measures how well the current input pattern matches the
			already trained pattern of the outgoing neuron.
			The \(\sigma_{ij} \) parameter is optional and allows to specify a significance for the synapse. An
			implementation may look like this:
		</p>
			$$\sigma_{ij} = (f_i f_j)^\eta$$
		<p>
			In this formula the values \(f_i \) and \(f_j \) are the activation frequencies of the input and the output
			neuron. \(\eta\) is used to adjust the strength of the significance parameter. <br/>
			Long-term Depression (LTD) comes into play when the incoming and outgoing neurons of a synapse do not fire
			at the same time. But the weakening effect of LTD depends on whether the synapse has an conjunctive or an
			disjunctive characteristic. Conjunctive synapses are only weakened when the input neuron is inactive and the
			output neuron is active. Disjunctive synapses are handled the other way around. <br/>
			Conjunctive synapses are usually used in excitatory neurons to detect a pattern. This means that if an input
			feature is not present in the training data set but the pattern neuron is activated nevertheless, that the
			responsible input synapse will be weakened. <br/>
			With disjunctive synapses the output neuron usually fire as soon as the input neuron is activated. But it
			might be suppressed by another negative input synapse. In this case the input neuron of a disjunctive synapse
			will fire but not the output neuron, which means the synapse will be weakened.
		</p>
			$$\forall_{i, j: x_i \leq 0, y_j > 0, disj(w_{ij})} \Delta w_{ij} = -\alpha \sigma_{ij} y_j$$
			$$\forall_{i, j: x_i > 0, y_j \leq 0, conj(w_{ij})} \Delta w_{ij} = -\alpha \sigma_{ij} x_i$$

		<p>
			Both LTP and LTD can be performed on a training document using the method LongTermLearning.train().
		</p>

		<h2>Meta-Synapses and Meta-Neurons</h2>
		<p>
			When training large neural networks, you quickly come across the problem that the number synapses grows
			quadratic with the number of neurons in a fully connected network. Even for a rather small network with a
			million neurons, the number of synapses would grow to up to a trillion if you were about to connect all
			neurons. Even testing if a synapse
			should be created between two neurons can easily become too expensive. It would also be desirable to generate
			new neurons only when they are needed instead of randomly before starting to train the network. <br/>
			To solve these problems, Aika introduces a meta layer to its network. The neurons and synapses from this meta
			layer are used to instantiate new neurons and synapses within Aikas neural network. The meta layer consists
			just of an additional set of parameters to some of the neurons and synapses.
			A meta-neuron can be activated just like any other neuron, but when the meta-neuron is activated a new neuron
			is generated from the meta parameters and meta-synapses of this neuron. To prevent that a new neuron is
			generated, even though another neuron representing this information already exists, a feedback loop with an
			inhibiting neuron is needed. This feedback loop allows to suppress the meta-neuron if the information that
			it is supposed to learn is already known to the network.
			The input synapses of the meta-neuron can be regular synapses, a combination of a regular synapse and a meta
			synapse or a pure meta-synapse. A pure meta-synapse would have no effect on the activation of the meta
			neuron. It is just used to create the new neuron.
		</p>
		<div style="text-align:center; width:100%">
			<img src="images/meta-network-1.svg" width="50%" height="50%" style="float:none; margin-left:auto; margin-right:auto; display:block"/>
		</div>
		<p>
			Another special feature of input meta-synapses is that when they connect to an inhibitory neuron, the resulting
			generated synapse will not connect to the inhibitory itself, but to the excitatory neuron that activated the
			inhibitory neuron. In this case the synapse between inhibitory neuron and the meta-neuron is positive, event
			though outgoing synapses of inhibitory neurons are usually negative. The purpose of this feature is to allow
			meta-synapses to define connections between
			abstract categories. However, the resulting synapse connects the concrete instances of these categories.
		</p>
		<div style="text-align:center; width:100%">
			<img src="images/meta-network-2.svg" width="45%" height="45%" style="float:none; margin-left:auto; margin-right:auto; display:block"/>
		</div>
		<p>
			Meta-neurons can be created using the method MetaNetwork.initMetaNeuron(). Synapses are added to this meta
			neuron in the following way:
		</p>
		<div class="prettyprint-code">
        	<pre class="prettyprint">
				<code class="language-java">
			new MetaSynapse.Builder()
				.setNeuron(wordInhib)
				.setWeight(20.0)
				.setBias(-20.0)
				.setRelativeRid(0)
				.setRangeMatch(EQUALS)
				.setRangeOutput(false),
				.setMetaWeight(20.0)
				.setMetaBias(-20.0)
				.setMetaRelativeRid(true)
        		</code>
			</pre>
		</div>
		<p>
			To generate new neurons and synapses based on an already processed training document, the method
			MetaNetwork.train() needs to be called.
		</p>

		<h2>Frequent Pattern Discovery</h2>

		<p>
			Since Aika is using a pattern lattice as underlying data structure for neurons, we might be able to use this
			pattern lattice as an unsupervised training method for the network.
			The pattern lattice allows to enumerate all frequent or otherwise statistically interesting patterns
			and use them to generate new neurons. Therefore, we do not have to retrain existing neurons and risk losing
			the conceptual knowledge they have already accumulated. <br/>
			The pattern discovery process can be started by calling the discover method in the PatternDiscovery class.
		</p>
	</div>

</div>
</body>
</html>