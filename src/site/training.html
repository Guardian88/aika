<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Training - Aika</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
	<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
</head>
<body>
<div id="header">
	<div>
		<div class="logo">
			<a href="index.html"></a>
		</div>
		<ul id="navigation">
			<li>
				<a href="index.html">Overall idea</a>
			</li>
			<li>
				<a href="inference.html">Inference</a>
			</li>
			<li class="active">
				<a href="training.html">Training</a>
			</li>
			<li>
				<a href="usage.html">Examples</a>
			</li>
			<li>
				<a href="resources.html">Resources</a>
			</li>
		</ul>
	</div>
</div>
<div id="contents">
	<div class="features">
		<h1>Training Mechanisms</h1>
		<p>
			The architectural framework of the Aika algorithm allows to implement a broad spectrum of diverse
			training mechanisms. These training mechanisms have their roots within the different artificial intelligence
			concepts that Aika brings together.
		</p>
		<h2>Supervised Learning with the Delta-Rule</h2>
		<p>
			A standard training method that can be used for the Aika network as well is the delta rule.
		</p>
		$$\Delta w_{ij} = \alpha (t_j - y_j) x_i$$
		<p>
			Where \(\alpha \) is the learn rate, \(t_j \) is the target output value, \(x_i \) is the input activation value
			and \(y_j \) is the actual output activation value.
		</p>

		<h2>Long-Term Potentiation and Long-Term Depression</h2>
		<p>
			Long-term potentiation and long-term depression are two learning mechanisms that occur naturally in the brain.
			Long-term potentiation (LTP) is the process of a persistent strengthening of synapses based on recent
			patterns of activity. These are patterns of synaptic activity that produce a long-lasting increase in signal
			transmission between two neurons. The opposite of LTP is long-term depression, which produces a long-lasting
			decrease in synaptic strength. <br/>

			LTP also gave rise to Hebbs learning rule which can be summarized by the expression "What fires together
			wires together". <br/>

			The following is a formulaic description of Hebbian learning:
		</p>
			$$\Delta w_{ij} = \alpha x_i y_j$$
		<p>
			Where \(\alpha \) is the learn rate, \(x_i \) is the input activation value and \(y_j \) is the output
			activation value.
			The weakness of Hebbs learning rule is that the strength of synapses will increase infinitely, which means
			that networks based on the Hebb rule are unstable.

		</p>
			$$\Delta w_{ij} = \alpha \sigma_{ij} x_i (1 - y_j) y_j {o_j \over g_j^+} $$

			$$g_j^+ = b_j + \sum\limits_{i=0,w_{ij} > 0.0}^N{w_{ij}}$$
		<p>
			Another learning mechanism that can be derived from the interpretations is the following. Since only the
			strongest interpretation is chosen
			and all other interpretations are suppressed, we have a convenient way to automatically generate our
			supervised training data.
		</p>


		<h2>Meta-Synapses and Meta-Neurons</h2>
		<p>
			When training large neural networks, you quickly come across the problem that the number synapses grows
			quadratic with the number of neurons. Even for a rather small network with a million neurons, the number of
			synapses would grow to up to a trillion if you were about to connect all neurons. Even testing if a synapse
			should be created between two neurons can easily become too expensive. It would also be desirable to generate
			new neurons only when they are needed instead of randomly before starting to train the network. <br/>
			To solve these problems, Aika introduces a meta layer to its network. The neurons and synapses from this meta
			layer are used to instantiate new neurons and synapses within Aikas neural network. The meta layer consists
			just of an additional set of parameters to some of the neurons and synapses.
		</p>

		<h2>Frequent Pattern Discovery</h2>

		<p>
			Since Aika is using a pattern lattice as underlying data structure for neurons, we might be able to use this
			pattern lattice as an unsupervised training method for the network.
			The pattern lattice allows to enumerate all frequent or otherwise statistically interesting patterns
			and use them to generate new neurons. Therefore, we do not have to retrain existing neurons and risk losing
			the conceptual knowledge they have already accumulated.
		</p>
	</div>

</div>
</body>
</html>