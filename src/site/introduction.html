<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>The Aika Algorithm for Text Mining and Information Extraction</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
</head>
<body>
<div id="header">
	<div>
		<div class="logo">
			<a href="index.html"></a>
		</div>
		<ul id="navigation">
			<li>
				<a href="index.html">Overall idea</a>
			</li>
			<li class="active">
				<a href="introduction.html">How it works</a>
			</li>
			<li>
				<a href="usage.html">Examples</a>
			</li>
			<li>
				<a href="resources.html">Resources</a>
			</li>
			<li>
				<a href="vision.html">Vision</a>
			</li>
		</ul>
	</div>
</div>
	<div id="contents">
		<div class="features">
			<h1>How it works</h1>
			<p>

			<h2>Artificial neural networks vs. frequent pattern mining</h2>
			<p>
				So how do an <a href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="nofollow">artificial neural network</a> and a <a href="http://www.charuaggarwal.net/freqbook.pdf" rel="nofollow">frequent pattern mining</a> algorithm fit together? Well, ANNs basically consist of neurons that compute an output value based on the weighted sum of their inputs.
				These neurons can be reformulated as a set of boolean operations.

				For example, the following <a href="https://en.wikipedia.org/wiki/Artificial_neuron" rel="nofollow">neuron</a> utilizing the <a href="https://de.wikipedia.org/wiki/Heaviside-Funktion" rel="nofollow">heaviside transfer function</a> (1 if x > 0 otherwise 0)
			</p>
			<img src="images/equ-1.svg"/>
			<img src="images/equ-2.svg"/>
			<p>
				can also be written as boolean equation
			</p>
			<img src="images/equ-3.svg"/>

			<p>
				So it is easy to see that the neuron's weighted sum can be reformulated as a <a href="https://en.wikipedia.org/wiki/Disjunctive_normal_form" rel="nofollow">disjunctive normal form</a> boolean equation, meaning that we always
				get a set of conjunctions that are connected by a disjunction. Now, the trick to bring the two machine learning approaches together is to store these conjunctions as nodes within a <a href="https://en.wikipedia.org/wiki/Lattice_(order)" rel="nofollow">pattern lattice</a>. A
				pattern lattice is a directed acyclic graph containing all subpatterns of a given pattern.
			</p>

			<img src="images/lattice.svg"/>

			<p>
				An advantage of this representation is that it greatly reduces the computational costs. In a classical
				neural network the weighted sums of all neurons would have to be computed. This requires
				a lot of computational power if the network is large. Aika on the other hand does not even touch a neuron if it
				has no chance of being activated. Activations must first pass a few layers of logic nodes within the
				pattern lattice before the activation of a neuron output is even considered. Each layer filters out activations that
				are unable to activate a neuron output. Though the depth of the pattern
				lattice is limited. Otherwise there would be combinatorial too many logic nodes.
			</p>

			<img src="images/network.svg"/>

			<h2>Activations</h2>
			<p>
				Another advantage of having a discrete representation of a neural network is that we are able to assign
				a specific text segment, a word position (relational id) and a
				specific interpretation option to each activation and propagate them along through the network. This accommodates for the
				fact that processing text is an inherently relational task. Words,
				phrases and sentences are in a relation to each other through their sequential order. The assignment of text
				ranges and word positions to activations is a simple yet powerful representation of the relational structure of text and avoids
				some of the shortcomings of other representations such as bag of words or sliding window. In Aika the
				synapses are able to manipulate the text range and the word position while the activation is passed on to the next neuron.
			</p>

			<h2>Interpretations</h2>
			<p>
				One common problem when processing text is that of cyclic dependencies. In the example 'jackson cook' it is impossible
				to decide which word has to be resolved first, the forename or the surname, since both depend on each other.
				To tackle this problem Aika employs <a href="https://en.wikipedia.org/wiki/Non-monotonic_logic" rel="nofollow">non-monotonic logic</a> and
				is therefore able to derive conclusions that are mutually exclusive. These conclusions are then weighted and only the strongest conclusion
				is returned as the result. Consider the following network as an example, in which the different linguistic concepts are represented by individual neurons.
			</p>
			<p>
				<img src="images/mutual-exclusion.svg"/>
			</p>

			<p>
				This network is able to determine whether a name, which has been recognized in a text, is a surname, city name, or profession. In this example network, we have omitted the common knowledge rules for simplicity's sake. If for instance the
				name "washington" has been recognized in a text, it will trigger further activations in the surname hint neuron and the city name hint neuron. These neurons are just disjunctions and do not yet decide whether this is a surname or a city name.
				In the next neural layer, however, we have negations that prevent that "washington" is classified as both surname and city. Nevertheless, the hint neurons will propagate their activations to the surname neuron and the city name neuron. But these activations
				will carry conflicting interpretation options and only one interpretation will be selected as the result later on.
			</p>
			<p>
				To avoid conflicting conclusions, Aika annotates all activations
				that are emitted by a neuron with a primitive option number. These primitive option numbers are then propagated through the network along with the neuron activations. If such an primitive option number arrives at the
				input of a negation node, a conflict is generated between the input option number (or the set of input option numbers) and the output option number generated by the current neuron. All activations
				whose set of option numbers contain such a conflict are then removed from the network.
			</p>

			<p>
				The primitive option numbers accumulate while they are propagated through the network. Such a set of primitive option numbers is then called an option.
				To be able to perform all kinds of set operations on these options, an option lattice is built.
			</p>

			<p>
				To determine the best interpretation of a given word, a good starting point is to simply select the option with the highest sum of associated activation weights. However, there might be better interpretations consisting of not just one but several options.
				These options must not conflict with each other, which makes it difficult to efficiently determine the best interpretation. In order to find the best interpretation, a binary search tree is traversed, in which one child node is an expansion
				of the so far determined interpretation and the other child node is the conflicting path. The search will retrieve the set of options with the highest weight sum. If no conflicts exists, this will simply be the set of all options.
			</p>


			<h2>Training</h2>
			<p>
				Currently Aika has only rudimentary support for automatic training. However, the approach that Aika takes allows
				to implement some learning mechanisms that would be difficult or impossible to implement otherwise.
				Since Aika is using a pattern lattice as underlying data structure for neurons, we do not depend solely on <a href="https://en.wikipedia.org/wiki/Backpropagation" rel="nofollow">backpropagation</a> as training method for new neurons.
				Now we are able to enumerate all frequent patterns and use them to generate new neurons. Therefore, we do not have to retrain existing neurons and risk losing the conceptual knowledge
				they have already accumulated. Frequent patterns are also much easier to comprehend for humans than the weights of conventional ANNs. This is a form of unsupervised learning.
			</p>
			<p>
				Another learning mechanism that can be derived from the interpretations is the following. Since only the strongest interpretation is chosen
				and all other interpretations are suppressed, we have an convenient way to automatically generate our supervised training data.
			</p>

		</div>
	</div>
</body>
</html>