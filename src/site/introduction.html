<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>The Aika Algorithm for Text Mining and Information Extraction</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
</head>
<body>
<div id="header">
	<div>
		<div class="logo">
			<a href="index.html"></a>
		</div>
		<ul id="navigation">
			<li>
				<a href="index.html">Overall idea</a>
			</li>
			<li class="active">
				<a href="introduction.html">How it works</a>
			</li>
			<li>
				<a href="usage.html">Examples</a>
			</li>
			<li>
				<a href="resources.html">Resources</a>
			</li>
			<li>
				<a href="vision.html">Vision</a>
			</li>
		</ul>
	</div>
</div>
	<div id="contents">
		<div class="features">
			<h1>How it works</h1>
			<p>

			<h2>Artificial neural networks vs. frequent pattern mining</h2>
			<p>
				So how do an <a href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="nofollow">artificial neural network</a> and a <a href="http://www.charuaggarwal.net/freqbook.pdf" rel="nofollow">frequent pattern mining</a> algorithm fit together? Well, ANNs basically consist of neurons that compute an output value based on the weighted sum of their inputs.
				These neurons can be reformulated as a set of boolean operations.

				For example, the following <a href="https://en.wikipedia.org/wiki/Artificial_neuron" rel="nofollow">neuron</a> utilizing the <a href="https://de.wikipedia.org/wiki/Heaviside-Funktion" rel="nofollow">heaviside transfer function</a> (1 if x > 0 otherwise 0)
			</p>
			<img src="images/equ-1.svg"/>
			<img src="images/equ-2.svg"/>
			<p>
				can also be written as boolean equation
			</p>
			<img src="images/equ-3.svg"/>

			<p>
				So it is easy to see that the neuron's weighted sum can be reformulated as a <a href="https://en.wikipedia.org/wiki/Disjunctive_normal_form" rel="nofollow">disjunctive normal form</a> boolean equation, meaning that we always
				get a set of conjunctions that are connected by a disjunction. Now, the trick to bring the two machine learning approaches together is to store these conjunctions as nodes within a <a href="https://en.wikipedia.org/wiki/Lattice_(order)" rel="nofollow">pattern lattice</a>. A
				pattern lattice is a directed acyclic graph containing all subpatterns of a given pattern.
			</p>

			<img src="images/lattice.svg"/>

			<p>
				An advantage of this representation is that it greatly reduces the computational costs. In a classical
				neural network the weighted sums of all neurons would have to be computed. This requires
				a lot of computational power if the network is large. Aika on the other hand does not even touch a neuron if it
				has no chance of being activated. Activations must first pass a few layers of logic nodes within the
				pattern lattice before the activation of a neuron output is even considered. Each layer filters out activations that
				are unable to activate a neuron output. Though the depth of the pattern
				lattice is limited. Otherwise too many logic nodes would be generated.
			</p>

			<img src="images/network.svg"/>

			<h2>Activations</h2>
			<p>
				Another advantage of having a discrete representation of a neural network is that we are able to assign
				a specific text segment, a word position (relational id) and a
				specific interpretation option to each activation and propagate them along through the network. This accommodates for the
				fact that processing text is an inherently relational task. Words,
				phrases and sentences are in a relation to each other through their sequential order. The assignment of text
				ranges and word positions to activations is a simple yet powerful representation of the relational structure of text and avoids
				some of the shortcomings of other representations such as bag of words or sliding window. In Aika the
				synapses are able to manipulate the text range and the word position while the activation is passed on to the next neuron.
			</p>

			<h2>Interpretations</h2>
			<p>
				One common problem when processing text is that of cyclic dependencies. In the example 'jackson cook' it is impossible
				to decide which word has to be resolved first, the forename or the surname, since both depend on each other. The word
				jackson can be recognized as a forename when the next word is a surname and the word cook can be recognized as a surname when the
				previous word is a forename.
				To tackle this problem Aika employs <a href="https://en.wikipedia.org/wiki/Non-monotonic_logic" rel="nofollow">non-monotonic logic</a> and
				is therefore able to derive interpretations that are mutually exclusive. These interpretations are then weighted and only the strongest interpretation
				is returned as the result. Consider the following network as an example, in which the different linguistic concepts are represented by individual neurons.
			</p>
			<p>
				<img src="images/example-diagram.svg"/>
			</p>

			<p>
				This network is able to determine whether a word, which has been recognized in a text, is a surname, city name, or profession. If for instance the
				word "jackson" has been recognized in a text, it will trigger further activations in the two jackson entity neurons. Since both are connected through the
				suppressing neuron only one of them can be active in the end. Aika will therefore generate two interpretations. But these
				interpretations are not limited to a single neuron. For instance if the word neuron cook gets activated too, then the jackson forename entity and the cook surname
				entity will be part of the same interpretation. The forename and surname category links in this example form a positive feedback loop which reinforces this interpretation.
			</p>
			<p>
				In order to be able to generate these interpretations, Aika annotates all activations
				that are emitted by a neuron with a primitive option number. These primitive option numbers are then propagated through the network along with the neuron activations. If such an primitive option number arrives at the
				input of a negation node, a conflict is generated between the input option number (or the set of input option numbers) and the output option number generated by the current neuron. All activations
				whose set of option numbers contain such a conflict are then removed from the network.
			</p>

			<p>
				The primitive option numbers accumulate while they are propagated through the network. Such a set of primitive option numbers is then called an option. To determine the best interpretation of a given word, a good starting point is to simply select the option with the highest sum of associated activation weights. However, there might be better interpretations consisting of not just one but several options.
				These options must not conflict with each other, which makes it difficult to efficiently determine the best interpretation. In order to find the best interpretation, a binary search tree is traversed, in which one child node is an expansion
				of the so far determined interpretation and the other child node is the conflicting path. The search will retrieve the set of options with the highest weight sum. If no conflicts exist, this will simply be the set of all options.
			</p>


			<h2>Training</h2>
			<p>
				Currently Aika only has rudimentary support for automatic training. However, the approach that Aika takes allows
				to implement some learning mechanisms that would be difficult or impossible to implement otherwise.
				Since Aika is using a pattern lattice as underlying data structure for neurons, we do not depend solely on <a href="https://en.wikipedia.org/wiki/Backpropagation" rel="nofollow">backpropagation</a> as a training method for new neurons.
				Now we are able to enumerate all frequent patterns and use them to generate new neurons. Therefore, we do not have to retrain existing neurons and risk losing the conceptual knowledge
				they have already accumulated. Frequent patterns are also much easier to comprehend for humans than the weights of conventional ANNs. This is a form of unsupervised learning.
			</p>
			<p>
				Another learning mechanism that can be derived from the interpretations is the following. Since only the strongest interpretation is chosen
				and all other interpretations are suppressed, we have a convenient way to automatically generate our supervised training data.
			</p>

		</div>
	</div>
</body>
</html>