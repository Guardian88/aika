<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Inference - Aika</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
	<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
</head>
<body>
<div id="header">
	<div>
		<div class="logo">
			<a rel="canonical" href="https://aika.network"></a>
		</div>
		<ul id="navigation">
			<li>
				<a rel="canonical" href="https://aika.network">Overall idea</a>
			</li>
			<li>
				<a rel="canonical" href="blog.html">Blog</a>
			</li>
			<li class="active">
				<a rel="canonical" href="inference.html">Inference</a>
			</li>
			<li>
				<a rel="canonical" href="training.html">Training</a>
			</li>
			<li>
				<a rel="canonical" href="usage.html">Examples</a>
			</li>
			<li>
				<a rel="canonical" href="resources.html">Resources</a>
			</li>
            <li>
                <a rel="canonical" href="https://github.com/aika-algorithm/aika">GitHub</a>
            </li>
		</ul>
	</div>
</div>
	<div id="contents">
		<div align="right"><span style='color: #FF0000;'>October 09, 2020</span></div>
		<div align="middle"><b><span style='color: #FF0000;'>Draft</span></b></div>
		<div class="features">
			<h1>How information is propagated through the network</h1>
			<p>
			<h2>General Architecture</h2>
			<p>
				Neurons and their Activations need to be represented separately. Each neuron may have one or more activations. Activations correspond to some pattern that has been observed in the real world that is in the input data set. For example, if the input data set is a text and a neuron represents a specific word, then there might be several occurrences of this word and therefore several activations of this neuron. The human brain probably does this through the timing of activation spikes.
				Activations are grounded in the input data set. This is where they derive their identity from.
				Since we have a one-to-n relation between neurons and activations, we also need a one-to-n relation between the synapses and the links between the activations. Therefore we need a linking process to determine which activation is connected to each other. Roughly speaking activations can be linked iff they are grounded in the same input data.
				A consequence of separating the neurons and their activations is that we can not rely on the network topology to give us the sequence in which each activation is processed. Therefore each activation needs a fired timestamp that describes the point in time when this activation becomes visible to other neurons.
				Initially, the network starts out empty. Neurons and synapses are added during training. There is an underlying set of rules which determine when certain types of neurons or synapses are induced.
			</p>
			<h2>Neuron Types</h2>
			<p>
				Also, neurons need to be able to act more like formal logic. Sure their ability to integrate weak input signals is important, this is something that classical AI has great difficulties with, but there is also the need for strong conjunctive or disjunctive behavior. Surely, these things can be achieved by setting the synapse weights and the bias value appropriately, but still, these neurons are sufficiently different to justify the introduction of different types of neurons for these. I believe that the human brain does something very similar with its excitatory pyramidal neurons and its inhibitory stellate neurons.
				Now to the part where I see the great similarity to your capsule networks. Basically, there are three types of neurons, pattern neurons, pattern part neurons, and inhibitory neurons. The pattern neurons and the pattern part neurons are both conjunctive in nature and the inhibitory neuron is disjunctive. The pattern part neurons kind of describe which lower-level patterns are part of the current pattern. Each pattern part neuron receives a positive feedback synapse from the pattern to which it belongs to. The pattern neuron on the other hand is only activated if the pattern as a whole is detected. For example, if we consider a word consisting of individual letters as lower-level patterns, then we have a corresponding pattern part neuron for each letter whose meaning is that this letter occurred as part of this word. The pattern part neurons also receive negative feedback synapses from the inhibitory neurons such that competing patterns are able to suppress each other.
			</p>
			<h2>Network Topology</h2>
			<p>
			<h2>Relations</h2>
			<p>
				It is easy to see that activations are able to represent real world objects, but that is not sufficient. We also need a way to represent relations between these objects. This can be achieved using a type of neurons whose synapses link two patterns together.
			</p>
			<h2>Positive and Negative Feedback Loops</h2>
			<p>
				Another crucial insight is the need for positive and negative feedback loops. These are synapses that ignore the causal sequence of fired activations. Especially the negative feedback synapses are interesting because they require the introduction of mutually shielded branches for the following activations. They create a kind of independent interpretation of parts of the input data set and only later on its decision which of these interpretations gets selected. It's very similar to what a parse tree does only that a parse tree is limited to syntactic information only. Another way to relate it to classical logic is to consider it as non-monotonic inference, which in classical AI could not be solved properly since classical logic is missing the weak influences that neural networks are able to capture.
			</p>
			<h2>The Artificial Neural Network</h2>
			<p>
				Aika is an artificial neural network, that uses neurons to represent a wide variety of
				linguistic concepts and connects them via synapses. In contrast to most other neural
				network architectures, Aika does not employ layers to structure its network. Synapses can connect arbitrary
				neurons with each other, but the network is not fully connected either. <br/>
				Like other artificial neural networks (ANN) the synapses are weighted. By choosing the weights and the threshold
				(i.e. the bias) accordingly, neurons can take on the characteristics of boolean logic gates such as an
				and-gate or an or-gate. To compute the activation value of a neuron, the weighted sum over its input
				synapses is computed. Then the bias value \(b\) is added to this sum and the result is
				sent through an activation function \(\varphi\).
            </p>
            <div style="text-align:center; width:100%">
                <img src="images/neuron.svg" width="60%" height="60%" style="float:none; margin-left:auto; margin-right:auto; display:block"/>
            </div>
            <p>
				$$net_j = {b_j + \sum\limits_{i=0}^N{x_i w_{ij}}}$$
				$$y_j = \varphi (net_j)$$

				Depending on the type of neuron, different activation functions are used. One commonly used activation function
				in Aika is the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified</a>
				hyperbolic tangent function, which is basically the positive half of the \(\tanh()\) function.

				\[\varphi(x) = \Bigg \{ {0 \atop \tanh(x)} {: x \leq 0 \atop : x > 0}\]
            </p>
            <div style="text-align:center; width:100%">
                <img src="images/tanh-diagram.png" width="50%" height="50%" style="float:none; margin-left:auto; margin-right:auto; display:block"/>
            </div>
            <p>
				The activation functions are chosen in a way that they clearly distinguish between active
				and inactive neurons. Only activated neurons are processed. These activations are expressed not only by a real
				valued number but also by an activation object.
			</p>

			<h2>Activations</h2>
			<p>
				The advantage of having activation objects is that, through them, Aika is able to cope with the relational
				structure of natural language text by making the activation relate to a specific segment of text. In a way
				these activations can be seen as text annotations, they are grounded in the input data set through their input links. </br>
			</p>



            <h2>Synapses</h2>
            <p>
                Synapses in Aika consist not just of a weight value but also properties that specify relations between
				synapses. These relations can either be used to imply an constrained on the matching input text ranges or
				the dependency structure of the input activations. Biological neurons seem to achieve such a relation
				matching through the timing of the firing patterns of their action potentials. <br/>
				The complete list of synapse properties looks as follows:
            </p>

                <div class="prettyprint-code">
                    <pre class="prettyprint">
				        <code class="language-java">
                new Synapse.Builder()
                        .setSynapseId(0)
                        .setNeuron(inputNeuronA)
                        .setWeight(10.0)
                        .setRecurrent(false)
                new Synapse.Builder()
                        .setSynapseId(1)
                        .setNeuron(inputNeuronB)
                        .setWeight(10.0)
                        .setRecurrent(false)
                new Relation.Builder()
                        .setFrom(0)
                        .setTo(1)
                        .setRelation(new Equals(END, BEGIN)),
                new Relation.Builder()
                        .setFrom(0)
                        .setTo(OUTPUT)
                        .setRelation(new Equals(BEGIN, BEGIN)),
                new Relation.Builder()
                        .setFrom(1)
                        .setTo(OUTPUT)
                        .setRelation(new Equals(END, END))
                        </code>
			        </pre>
                </div>
            <p>
                Unlike other ANNs, Aika allows to specify a bias value per synapse. These bias values are simply summed
                up and added to the neurons bias value. <br/>
                The property recurrent states whether this synapse is a feedback loop or not. Depending on the weight
                of this synapse such a feedback loop might either be positive or negative. This property is an important
                indicator for the interpretation search. <br/>
                Range relations define a relation either to another synapse or to the output range of this activation.
				The range relations consist of four comparison operations between the between the begin and end of the
				current synapses input activation and the linked synapses input activation.
            </p>

            <div style="text-align:center; width:100%">
                <img src="images/range-match.svg" style="float:none; margin-left:auto; margin-right:auto; display:block"/>
            </div>
            <p>
                The next property is the range output which consists of two boolean values. If these are set to true,
                the range begin and the range end are propagated to the output activation. <br/>

				The last two properties are used to establish a dependency structure among activations. The instance
				relation here defines whether this synapse and the linked synapse have a common ancestor or are
				depending on each other in either direction. During the evaluation of this relation only those synapses
				are followed which are marked with the identity flag.

				Depending on the weight of the synapse and the bias sum of the outgoing neuron, Aika distinguishes two
				types synapses: conjunctive synapses and disjunctive synapses. Conjunctive synapses are stored in the
				output neuron as inputs and disjunctive synapses are stored in the input neuron as outputs. The reason
				for this is that disjunctive neurons like the inhibitory neuron or the category neurons may have a huge
				number of input synapses, which makes it expensive to load them from disk. By storing those synapses on
				the input neuron side, only those synapses need to stay in memory, that are needed by an currently
				activated input neuron.
            </p>

			<h2>Neuron Types</h2>
			<p>
				There are two main types of neurons in Aika: excitatory neurons and inhibitory neurons. The biological
				role models for those neurons are the spiny pyramidal cell and the aspiny stellate cell in the cerebral
				cortex. The pyramidal cells usually exhibit an excitatory characteristic and some of them possess long
				ranging axons that connect to other parts of the brain. The stellate cells on the other hand are usually
				inhibitory interneurons with short axons which form circuits with nearby neurons. <br/>
				Those two types of neurons also have a different electrical signature. Stellate cells usually react to
				a constant depolarising current by firing action potentials. This occurs with a relatively constant frequency
				during the entire stimulus. In contrast, most pyramidal cells are unable to maintain a constant firing
				rate. Instead, they are firing quickly at the beginning of the stimulus and then reduce the frequency even
				if the stimulus stays strong. This slowdown over time is called adaption. <br/>
				Aika tries to mimic this behaviour by using different activation functions for the different types of
				neurons. Since Aika is not a spiking neural network like the biological counterpart, we only have the neurons
				activation value which can roughly be interpreted as the firing frequency of a spiking neuron. In a sense the
				earlier described activation function based on the rectified tanh function quite nicely captures
				the adaption behaviour of a pyramidal cell. An increase of a weak signal has a strong effect on the
				neurons output, while an increase on an already strong signal has almost no effect. Furthermore, if the
				input of the neuron does not surpass a certain threshold then the neuron will not fire at all.
				For inhibitory neurons Aika uses the rectified linear unit function (ReLU).
			</p>

				$$y = \max(0, x)$$

			<div style="text-align:center; width:100%">
				<img src="images/relu-diagram.png" width="50%" height="50%" style="float:none; margin-left:auto; margin-right:auto; display:block"/>
			</div>
			<p>
				Especially for strongly disjunctive neurons like the inhibitory neuron, ReLU has the advantage of
				propagating its input signal exactly as it is, without distortion or loss of information.
			</p>
        </div>
	</div>
</body>
</html>